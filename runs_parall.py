import itertools
import pandas as pd
import numpy as np
import torch
#from kan import KAN
from kan.utils import create_dataset_from_data, ex_round
from joblib import Parallel, delayed
import multiprocessing
import time
from logs import *
from save import *
import math
from tqdm import tqdm


# === –§—É–Ω–∫—Ü–∏–∏ ===
def f1(x): return np.sin(np.pi * x) + x**2
def f2(x): return np.exp(-x**2)
def f3(x): return x * np.sin(5 * x)
def f4(x): return np.sin(np.pi * x[:, 0]) + x[:, 1]**2
def f5(x): return np.exp(np.sin(np.pi * x[:, 0]) + x[:, 1]**2)

FUNCTIONS = [
    (f1, 1, "sin(pi*x) + x^2", "D1N1"),
    (f2, 1, "exp(-x^2)", "D1N2"),
    (f3, 1, "x * sin(5*x)", "D1N3"),
    (f4, 2, "sin(pi*x) + y^2", "D2N1"),
    (f5, 2, "exp(sin(pi*x) + y^2)", "D2N2"),
]
lib = ['x','x^2','x^3','x^4','exp','log','sqrt','tanh','sin','abs']

# n_samples_list = [100, 200, 400, 500]
# noise_levels = [0, 0.05, 0.1, 0.3, 0.5]
# #gap_ranges = [None, [-1, 1], [0.5, 1]]
# gap_ranges = [None]


# width_list = [[5, 1],[4, 4, 4, 1],[15, 1],[5, 5, 1],[10, 6, 3, 1]]
# k_list = [1, 2, 3, 5]
# grid_list = [3, 5]

n_samples_list = [100]
noise_levels = [0, 0.05]
#gap_ranges = [None, [-1, 1], [0.5, 1]]
gap_ranges = [None]


width_list = [[5, 1],[4, 4, 4, 1]]
k_list = [1]
grid_list = [3]


# === –ù–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ ===
n_repeats = 5  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—É—Å–∫–æ–≤ –Ω–∞ –æ–¥–Ω—É –∫–æ–º–±–∏–Ω–∞—Ü–∏—é
x_true = np.linspace(-2, 2, 500)

# === –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã ===
def extract_loss_history(fit_result, key):
    if fit_result is None or key not in fit_result:
        return []
    val = fit_result[key]
    if isinstance(val, (list, np.ndarray)):
        return str([float(t) for t in val])
    elif isinstance(val, (float, int)):
        return [val]
    return []

def stats(model, dataset, func, n_var, x_true):
    x_true_clear = x_true.copy()
    if n_var == 2:
        x_true_clear = np.column_stack([x_true, x_true])
    y_true_clear = func(x_true_clear)

    with torch.no_grad():
        device = model.device
        y_pred = model(dataset['test_input'])
        y_true = dataset['test_label']
        y_true_pred = model(
            torch.tensor(x_true_clear, dtype=torch.float32, device=device).reshape((len(x_true), n_var))
        )

    y_pred_np = y_pred.cpu().numpy().flatten()
    y_true_np = y_true.cpu().numpy().flatten()
    y_true_pred_np = y_true_pred.cpu().numpy().flatten()

    residuals = y_true_np - y_pred_np
    mse = np.mean(residuals ** 2)
    mae = np.mean(np.abs(residuals))
    ss_res = np.sum(residuals ** 2)
    ss_tot = np.sum((y_true_np - np.mean(y_true_np)) ** 2)
    r2 = 1 - (ss_res / (ss_tot + 1e-12))

    residuals_true = y_true_clear - y_true_pred_np
    mse_true = np.mean(residuals_true ** 2)
    mae_true = np.mean(np.abs(residuals_true))
    ss_res_true = np.sum(residuals_true ** 2)
    ss_tot_true = np.sum((y_true_clear - np.mean(y_true_clear)) ** 2)
    r2_true = 1 - (ss_res_true / (ss_tot_true + 1e-12))

    return {
        'test_residuals': residuals,
        'test_mse': mse,
        'test_mae': mae,
        'test_r2': r2,
        'true_residuals': residuals_true,
        'true_mse': mse_true,
        'true_mae': mae_true,
        'true_r2': r2_true,
    }

def run_single_experiment(args):
    (
        func, n_var, func_id, id_name,
        gap_range, n_samples, noise_level,
        width_tail, k, grid
    ) = args
    import kan

    # –ü–æ–ª–Ω–æ–µ –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    def dummy_log_history(self, method_name): pass
    def dummy_saveckpt(self, path): pass
    kan.KAN.log_history = dummy_log_history
    kan.KAN.saveckpt = dummy_saveckpt
    
    seed = np.random.randint(0, 2**30)
    gap_str = f"gap{gap_range[0]}_{gap_range[1]}" if gap_range else "nogap"
    width_current = [n_var] + width_tail
    filename = f'datasets/data_{id_name}_noise{noise_level}_{gap_str}_n{n_samples}.csv'

    try:
        df = pd.read_csv(filename, index_col=False)
    except FileNotFoundError:
        return None

    device = torch.device('cpu')
    if n_var == 1:
        x = torch.tensor(df['x1'].values, dtype=torch.float32).reshape(-1, 1)
    else:
        x = torch.tensor(df[['x1', 'x2']].values, dtype=torch.float32)
    y = torch.tensor(df['y'].values, dtype=torch.float32).reshape(-1, 1)
    dataset = create_dataset_from_data(x, y)

    model = kan.KAN(width=width_current.copy(), grid=grid, k=k, seed=seed, device=device, auto_save=False)

    # === –§—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ NaN –≤ –∏—Å—Ç–æ—Ä–∏–∏ –ª–æ—Å—Å–∞ ===
    def has_nan(history):
        if history is None:
            return True
        if isinstance(history, (list, np.ndarray)):
            return any(np.isnan(v) for v in history if v is not None)
        return np.isnan(history)

    # –≠—Ç–∞–ø 1: –¥–æ prune
    res_before = model.fit(dataset, opt="LBFGS", steps=20, lamb=0.001, update_grid=False)
    if res_before is None or has_nan(res_before['train_loss']) or has_nan(res_before['test_loss']) or has_nan(res_before['reg']):
        return None
    stats_before = stats(model, dataset, func, n_var, x_true)

    # –≠—Ç–∞–ø 2: –ø–æ—Å–ª–µ prune
    try:
        model = model.prune()
    except Exception as e:
        if "non-empty TensorList" in str(e) or "in_dim" in str(e):
            return None
        else:
            raise e
    res_after_prune = model.fit(dataset, opt="LBFGS", steps=20, update_grid=False)
    if res_after_prune is None or has_nan(res_before['train_loss']) or has_nan(res_before['test_loss']) or has_nan(res_before['reg']):
        return None
    stats_after_prune = stats(model, dataset, func, n_var, x_true)

    # –≠—Ç–∞–ø 3: –ø–æ—Å–ª–µ refine
    model = model.refine(10)
    res_after_refine = model.fit(dataset, opt="LBFGS", steps=20, update_grid=False)
    if res_after_refine is None or has_nan(res_before['train_loss']) or has_nan(res_before['test_loss']) or has_nan(res_before['reg']):
        return None
    stats_after_refine = stats(model, dataset, func, n_var, x_true)

    # –≠—Ç–∞–ø 4: symbolic
    model.auto_symbolic(lib=lib, verbose=False)
    res_after_symbolic = model.fit(dataset, opt="LBFGS", steps=20, update_grid=False)
    if res_after_symbolic is None or has_nan(res_before['train_loss']) or has_nan(res_before['test_loss']) or has_nan(res_before['reg']):
        return None
    stats_after_symbolic = stats(model, dataset, func, n_var, x_true)

    formula = str(ex_round(model.symbolic_formula()[0][0], 4))

    row = {
        'func_id': id_name,
        'noise_level': noise_level,
        'gap_str': gap_str,
        'n_samples': n_samples,
        'width': str(width_current),
        'k': k,
        'grid': grid,
        'seed': seed,
        'formula': formula,
        'loss_train_history_before_prune': extract_loss_history(res_before, 'train_loss'),
        'loss_test_history_before_prune': extract_loss_history(res_before, 'test_loss'),
        'reg_history_before_prune': extract_loss_history(res_before, 'reg'),
        'loss_train_history_after_prune': extract_loss_history(res_after_prune, 'train_loss'),
        'loss_test_history_after_prune': extract_loss_history(res_after_prune, 'test_loss'),
        'reg_history_after_prune': extract_loss_history(res_after_prune, 'reg'),
        'loss_train_history_after_refine': extract_loss_history(res_after_refine, 'train_loss'),
        'loss_test_history_after_refine': extract_loss_history(res_after_refine, 'test_loss'),
        'reg_history_after_refine': extract_loss_history(res_after_refine, 'reg'),
        'loss_train_history_after_symbolic': extract_loss_history(res_after_symbolic, 'train_loss'),
        'loss_test_history_after_symbolic': extract_loss_history(res_after_symbolic, 'test_loss'),
        'reg_history_after_symbolic': extract_loss_history(res_after_symbolic, 'reg'),
    }

    for stage_name, stat_dict in [
        ('before_prune', stats_before),
        ('after_prune', stats_after_prune),
        ('after_refine', stats_after_refine),
        ('after_symbolic', stats_after_symbolic),
    ]:
        for key in ['test_residuals', 'test_mse', 'test_mae', 'test_r2',
                    'true_residuals', 'true_mse', 'true_mae', 'true_r2']:
            row[f'{key}_{stage_name}'] = stat_dict[key]

    return row

# === –°—Ç–æ–ª–±—Ü—ã ===
base_cols = [
    'func_id', 'noise_level', 'gap_str', 'n_samples',
    'width', 'k', 'grid', 'seed', 'formula'  # –¥–æ–±–∞–≤–∏–ª–∏ seed
]
stages = ['before_prune', 'after_prune', 'after_refine', 'after_symbolic']
loss_cols = []
metric_cols = []
for stage in stages:
    loss_cols += [
        f'loss_train_history_{stage}',
        f'loss_test_history_{stage}',
        f'reg_history_{stage}',
    ]
    metric_cols += [
        f'test_residuals_{stage}',
        f'test_mse_{stage}',
        f'test_mae_{stage}',
        f'test_r2_{stage}',
        f'true_residuals_{stage}',
        f'true_mse_{stage}',
        f'true_mae_{stage}',
        f'true_r2_{stage}',
    ]
all_columns = base_cols + loss_cols + metric_cols


from joblib import Parallel, delayed
from tqdm import tqdm
import time
import os
import math

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
MAX_RETRIES = 3
TIMEOUT_SECONDS = 200
BATCH_SIZE = 36
n_jobs = 12

# –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
RESULTS_DIR = "results"

def is_batch_completed(id_name, batch_num):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –ª–∏ –ø–∞—á–∫–∞
    """
    parquet_path = f"{RESULTS_DIR}/results_{id_name}_{batch_num}.pkl"
    return os.path.isfile(parquet_path)


for func_idx, (func, n_var, name, id_name) in enumerate(FUNCTIONS):
    print(f"\nüöÄ –ó–∞–ø—É—Å–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏: {id_name}")
    v_log(f'–§—É–Ω–∫—Ü–∏—è {id_name} –Ω–∞—á–∞—Ç–∞')

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á (–∫–∞–∫ —Ä–∞–Ω—å—à–µ)
    tasks = []
    for gap_range in gap_ranges:
        for n_samples in n_samples_list:
            for noise_level in noise_levels:
                for width in width_list:
                    for k in k_list:
                        for grid in grid_list:
                            for i in range(n_repeats):
                                tasks.append((
                    func, n_var, func_idx, id_name,
                    gap_range, n_samples, noise_level,
                    width, k, grid
                ))

    total = len(tasks)
    print(f"–í—Å–µ–≥–æ –∑–∞–¥–∞—á –¥–ª—è {id_name}: {total}")
    n_batches = math.ceil(total / BATCH_SIZE)
    print(f"–†–∞–∑–±–∏—Ç–æ –Ω–∞ {n_batches} –ø–∞—á–µ–∫ –ø–æ {BATCH_SIZE} –∑–∞–¥–∞—á")


    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—á–∫–∞–º–∏
    for batch_idx in range(n_batches):
        start_idx = batch_idx * BATCH_SIZE
        end_idx = min(start_idx + BATCH_SIZE, total)
        batch_tasks = tasks[start_idx:end_idx]
        batch_num = batch_idx + 1
        batch_total = len(batch_tasks)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –ª–∏ —ç—Ç–∞ –ø–∞—á–∫–∞
        if is_batch_completed(id_name, batch_num):
            print(f"‚úÖ –ü–∞—á–∫–∞ {batch_num}/{n_batches} —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ (—Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç). –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
            v_log(f'{id_name} –ü–∞—á–∫–∞ {batch_num} –ø—Ä–æ–ø—É—â–µ–Ω–∞: —Ñ–∞–π–ª —É–∂–µ –µ—Å—Ç—å')
            continue

        print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—á–∫–∏ {batch_num}/{n_batches} ({batch_total} –∑–∞–¥–∞—á)")


        # –¶–∏–∫–ª –ø–æ–ø—ã—Ç–æ–∫ —Å –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–æ–º –ø—Ä–∏ —Ç–∞–π–º–∞—É—Ç–µ
        batch_results = None
        for attempt in range(1, MAX_RETRIES + 1):
            try:
                print(f"!–ü–æ–ø—ã—Ç–∫–∞ {attempt}/{MAX_RETRIES} –¥–ª—è –ø–∞—á–∫–∏ {batch_num}")
                batch_results = Parallel(n_jobs=n_jobs, timeout=TIMEOUT_SECONDS, verbose=5)(
                    delayed(run_single_experiment)(task)
                    for task in tqdm(batch_tasks, desc=f"–ü–∞—á–∫–∞ {batch_num} {id_name}", total=batch_total)
                )
                v_log(f'{id_name} –ü–∞—á–∫–∞ {batch_num} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {attempt} –ø–æ–ø—ã—Ç–∫—É. –†–µ–∑—É–ª—å—Ç–∞—Ç: {len(batch_results)} –∑–∞–ø–∏—Å–µ–π')
                break  # –£—Å–ø–µ—Ö ‚Äî –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞ –ø–æ–ø—ã—Ç–æ–∫

            except multiprocessing.TimeoutError:
            # –û—Å–Ω–æ–≤–Ω–æ–π —Å–ª—É—á–∞–π: —Ç–∞–π–º–∞—É—Ç joblib
                print(f"!‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç! –ü–∞—á–∫–∞ {batch_num}, –ø–æ–ø—ã—Ç–∫–∞ {attempt}")
                v_log(f'{id_name} –ü–∞—á–∫–∞ {batch_num}, –ø–æ–ø—ã—Ç–∫–∞ {attempt} ‚Äî TimeoutError')
                if attempt < MAX_RETRIES:
                    print(f"–ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –ø–∞—á–∫–∏ {batch_num} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})...")
                else:
                    print(f"‚ùå –í—Å–µ {MAX_RETRIES} –ø–æ–ø—ã—Ç–æ–∫ –∏—Å—á–µ—Ä–ø–∞–Ω—ã –¥–ª—è –ø–∞—á–∫–∏ {batch_num}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
                    v_log(f'{id_name} –ü–∞—á–∫–∞ {batch_num}: –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ—É–¥–∞—á–Ω—ã, –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç')
                    batch_results = []  # –ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏



        # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ—Ç (–≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        if batch_results is None or not batch_results:
            print(f"‚ö†Ô∏è –ü–∞—á–∫–∞ {batch_num}: –Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ—Å–ª–µ {MAX_RETRIES} –ø–æ–ø—ã—Ç–æ–∫")
            v_log(f'{id_name} –ü–∞—á–∫–∞ {batch_num}: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è')
            continue

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è None
        batch_rows = [r for r in batch_results if r is not None]
        v_log(f'{id_name} –ü–∞—á–∫–∞ {batch_num} –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°—Ç—Ä–æ–∫: {len(batch_rows)}')


        if not batch_rows:
            print(f"‚ö†Ô∏è –ü–∞—á–∫–∞ {batch_num}: –Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
            v_log(f'{id_name} –ü–∞—á–∫–∞ {batch_num}: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏')
            continue

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–∞—á–∫–∏
        batch_base_path = f"{RESULTS_DIR}/results_{id_name}_{batch_num}"
        save_result = save_experiment_data(
            rows=batch_rows,
            base_path=batch_base_path,
            protocol=4,
            parquet_compression="snappy"
        )
        v_log(f'{id_name} –ü–∞—á–∫–∞ {batch_num} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {save_result}')

        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ CSV
        # df_batch = pd.DataFrame(batch_rows)
        # csv_file = f"{RESULTS_DIR}/results_{id_name}_{batch_num}.csv"
        # df_batch.to_csv(csv_file, index=False, sep=';')
        # print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {csv_file} ({len(batch_rows)} —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤)")
        # v_log(f'{id_name} –ü–∞—á–∫–∞ {batch_num} CSV —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {csv_file}')

print("–í—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
